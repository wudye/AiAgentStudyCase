"""
æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ - åŸºäº LangGraph + Ollama æœ¬åœ°æ¨¡å‹çš„å¯¹è¯ç³»ç»Ÿ
ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ— éœ€å¤–éƒ¨ API
"""

import asyncio
from typing import TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰çŠ¶æ€ç»“æ„
class ChatState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str        # ç”¨æˆ·æŸ¥è¯¢
    final_answer: str      # æœ€ç»ˆç­”æ¡ˆ
    step: str             # å½“å‰æ­¥éª¤

# åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹
llm = ChatOpenAI(
    model=os.getenv("LLM_MODEL_ID", "llama3.1:8b"),
    api_key=os.getenv("LLM_API_KEY", "ollama"),
    base_url=os.getenv("LLM_API_BASE_URL", "http://localhost:11434/v1"),
    temperature=0.7
)

def generate_answer_node(state: ChatState) -> ChatState:
    """ç›´æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹å›ç­”ç”¨æˆ·é—®é¢˜"""
    
    # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break
    
    # æ„å»ºå¯¹è¯æç¤º
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œè¯·æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–ä»£ç ç¤ºä¾‹ã€‚
å›ç­”è¦ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£ã€‚"""

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ])
    
    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

# æ„å»ºå¯¹è¯å·¥ä½œæµ
def create_chat_assistant():
    workflow = StateGraph(ChatState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("answer", generate_answer_node)
    
    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "answer")
    workflow.add_edge("answer", END)
    
    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ™ºèƒ½å¯¹è¯åŠ©æ‰‹"""
    
    app = create_chat_assistant()
    
    print("ğŸ’¬ æ™ºèƒ½å¯¹è¯åŠ©æ‰‹å¯åŠ¨ï¼")
    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {os.getenv('LLM_MODEL_ID', 'llama3.1:8b')}")
    print("æ”¯æŒå„ç§é—®é¢˜ï¼šçŸ¥è¯†é—®ç­”ã€æŠ€æœ¯é—®é¢˜ã€å¯¹è¯äº¤æµç­‰")
    print("(è¾“å…¥ 'quit' é€€å‡º)\n")
    
    session_count = 0
    
    while True:
        user_input = input("ğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        
        if user_input.lower() in ['quit', 'q', 'é€€å‡º', 'exit']:
            print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
            break
        
        if not user_input:
            continue
        
        session_count += 1
        config = {"configurable": {"thread_id": f"chat-session-{session_count}"}}
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": user_input,
            "final_answer": "",
            "step": "start"
        }
        
        try:
            print("\n" + "="*60)
            
            # æ‰§è¡Œå·¥ä½œæµ
            async for output in app.astream(initial_state, config=config):
                for node_name, node_output in output.items():
                    if "messages" in node_output and node_output["messages"]:
                        latest_message = node_output["messages"][-1]
                        if isinstance(latest_message, AIMessage):
                            print(f"\nğŸ’¡ å›ç­”:\n{latest_message.content}")
            
            print("\n" + "="*60 + "\n")
        
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚\n")

if __name__ == "__main__":
    asyncio.run(main())